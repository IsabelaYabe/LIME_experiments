{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Isas_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importações de bibliotecas padrão\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importações de bibliotecas do scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Importações de bibliotecas do NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Importações do LIME\n",
    "import lime\n",
    "import lime.lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Importações do SciPy\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Importações de funções personalizadas (certifique-se de que os arquivos lime_functions.py e submodular.py estejam no diretório atual)\n",
    "import lime_functions\n",
    "import submodular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sep( n: int = 120, sep: str = \"=\"):\n",
    "    print(sep*n)\n",
    "\n",
    "def new_section(string: str, n: int = 120, sep: str = \"=\"):\n",
    "    m = len(string)\n",
    "    if n < m:\n",
    "        n = m + 6\n",
    "    padding = (n-m+1)//2\n",
    "    print(sep*padding + \" \" + string + \" \" + sep*padding)\n",
    "\n",
    "def subsection(string: str, n: int = 120, sep: str = \"-\"):\n",
    "    print()\n",
    "    print(string, end=\"\")\n",
    "    print(sep*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================ Definindo sentenças para análise ============================================\n",
      "Instância negativa: Nunca vi algo tão ruim na minha vida. A produção foi amadora e a história, entediante.\n",
      "Rótulo negativa: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Instância positiva: Este filme foi absolutamente incrível! A trama é envolvente e as atuações foram dignas de um Oscar.\n",
      "Rótulo positiva: 1\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "new_section(\"Definindo sentenças para análise\")\n",
    "# Exemplo de conjunto de dados de resenhas\n",
    "dados = pd.read_csv('dados.csv', sep=';')\n",
    "\n",
    "# Separar as features e o alvo\n",
    "X = dados['review']\n",
    "y = dados['sentimentos']\n",
    "\n",
    "# Escolher uma instância negativa para fazer a previsão\n",
    "instance_index_neg = 15\n",
    "instance_neg = X.iloc[instance_index_neg]\n",
    "instance_label_neg = y.iloc[instance_index_neg]\n",
    "print('Instância negativa:', instance_neg)\n",
    "print('Rótulo negativa:', instance_label_neg)\n",
    "sep(sep=\"-\")\n",
    "# Escolher uma instância negativa para fazer a previsão\n",
    "instance_index_pos = 0\n",
    "instance_pos = X.iloc[instance_index_pos]\n",
    "instance_label_pos = y.iloc[instance_index_pos]\n",
    "print('Instância positiva:', instance_pos)\n",
    "print('Rótulo positiva:', instance_label_pos)\n",
    "sep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deixar stopwords\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirar stop-words\n",
    "stop_words = stopwords.words('portuguese')\n",
    "X_sw = X.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Vetorização do texto\n",
    "vectorizer_sw = TfidfVectorizer()\n",
    "X_vectorized_sw = vectorizer_sw.fit_transform(X_sw)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train_sw, X_test_sw, y_train, y_test = train_test_split(X_vectorized_sw, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================= Testes, explorando X_vectorized =============================================\n",
      "\n",
      "X_vectorized.shape------------------------------------------------------------------------------------------------------------------------\n",
      "(98, 256)\n",
      "\n",
      "X_vectorized[instance_index_neg].indices------------------------------------------------------------------------------------------------------------------------\n",
      "[118 126 218 174 250   5 245 164 158 252 200   7  89]\n",
      "\n",
      "X_vectorized[instance_index_pos].tocoo().data------------------------------------------------------------------------------------------------------------------------\n",
      "[0.26188687 0.14662237 0.18251715 0.3096147  0.2267131  0.25127256\n",
      " 0.28980586 0.28980586 0.28980586 0.33753369 0.33753369 0.18624005\n",
      " 0.1943502  0.33753369]\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Testes, explorando X_vectorized\n",
    "new_section(\"Testes, explorando X_vectorized\")\n",
    "subsection(\"X_vectorized.shape\")\n",
    "print(X_vectorized.shape)\n",
    "subsection(\"X_vectorized[instance_index_neg].indices\")\n",
    "print(X_vectorized[instance_index_neg].indices)\n",
    "subsection(\"X_vectorized[instance_index_pos].tocoo().data\")\n",
    "print(X_vectorized[instance_index_pos].tocoo().data)\n",
    "#subsection(\"X_vectorized[instance_index_pos].tocoo()\")\n",
    "#print(X_vectorized[instance_index_pos].tocoo())\n",
    "#subsection(\"X_vectorized[instance_index_pos].tocoo().col\")\n",
    "#print(X_vectorized[instance_index_pos].tocoo().col)\n",
    "#subsection(\"X_vectorized[instance_index_pos][0]\")\n",
    "#print(X_vectorized[instance_index_pos][0]) # Igual a X_vectorized[instance_index_pos].tocoo()\n",
    "sep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================== Testes, explorando vectorizer ==============================================\n",
      "\n",
      "vectorizer.get_feature_names_out()[0]------------------------------------------------------------------------------------------------------------------------\n",
      "absolutamente\n",
      "\n",
      "vectorizer.get_feature_names_out()[1]------------------------------------------------------------------------------------------------------------------------\n",
      "achei\n",
      "\n",
      "vectorizer.get_feature_names_out()[2]------------------------------------------------------------------------------------------------------------------------\n",
      "acrescentaram\n",
      "\n",
      "vectorizer.get_feature_names_out()------------------------------------------------------------------------------------------------------------------------\n",
      "['absolutamente' 'achei' 'acrescentaram' 'adorei' 'ainda' 'algo'\n",
      " 'altamente' 'amadora' 'amei' 'anterior' 'ao' 'aos' 'apreciam' 'arte'\n",
      " 'artificiais' 'as' 'assisti' 'assistindo' 'assistir' 'atento' 'atenção'\n",
      " 'ator' 'atores' 'atuação' 'atuações' 'até' 'bastante' 'bela' 'belas'\n",
      " 'bem' 'bom' 'brilhante' 'cabeça' 'cada' 'cadeira' 'captura' 'cativam'\n",
      " 'cativante' 'cena' 'cenas' 'chato' 'chatos' 'cheio' 'cinema'\n",
      " 'cinematográfica' 'com' 'começo' 'complementa' 'conectar' 'confusa'\n",
      " 'confuso' 'consegui' 'cuidadosamente' 'da' 'de' 'decepcionante' 'deixa'\n",
      " 'deixou' 'deles' 'desde' 'desejar' 'desenvolvida' 'desenvolvidos'\n",
      " 'desinteressados' 'desinteressante' 'desperdício' 'deste' 'detalhe'\n",
      " 'dias' 'dignas' 'direção' 'dirigido' 'diálogos' 'do' 'dos' 'drama'\n",
      " 'efeitos' 'elaborada' 'em' 'emocional' 'emocionante' 'emocionantes'\n",
      " 'emocionaram' 'emoção' 'encantador' 'encontrar' 'enredo' 'enrendo'\n",
      " 'entediado' 'entediante' 'entender' 'entregaram' 'envolvente' 'envolver'\n",
      " 'eram' 'escrito' 'especiais' 'espectador' 'esperava' 'espetacular'\n",
      " 'essência' 'este' 'eu' 'excepcionais' 'executado' 'experiência'\n",
      " 'extremamente' 'fantástica' 'fantástico' 'fascinante' 'faz' 'feito'\n",
      " 'feitos' 'filme' 'filmes' 'fim' 'final' 'fiquei' 'foi' 'foram'\n",
      " 'fotografia' 'fraca' 'fraco' 'gostar' 'gostei' 'graça' 'história'\n",
      " 'horrível' 'humano' 'impecáveis' 'impecável' 'importar' 'impossível'\n",
      " 'incrível' 'inesperadas' 'inesquecível' 'ingresso' 'inspiradora'\n",
      " 'intrigante' 'início' 'irritantes' 'isso' 'jornada' 'já' 'lento' 'lindo'\n",
      " 'magnífico' 'mais' 'mal' 'mantiveram' 'mantêm' 'maravilhosa'\n",
      " 'maravilhoso' 'me' 'melhores' 'memoráveis' 'mensagem' 'merece' 'minha'\n",
      " 'minuto' 'momento' 'momentos' 'motivação' 'muito' 'na' 'nada' 'narrativa'\n",
      " 'nem' 'nenhum' 'nenhuma' 'neste' 'ninguém' 'no' 'novo' 'nunca' 'não'\n",
      " 'obra' 'os' 'oscar' 'para' 'parar' 'pareciam' 'pena' 'perca' 'perda'\n",
      " 'perfeitamente' 'performances' 'personagens' 'pior' 'piores' 'planejada'\n",
      " 'ponta' 'pontos' 'por' 'porcaria' 'prende' 'prendeu' 'previsível' 'prima'\n",
      " 'produzido' 'produção' 'profunda' 'profundidade' 'prêmios' 'pé' 'péssimo'\n",
      " 'público' 'qualidade' 'que' 'raro' 'realmente' 'recomendo' 'recomendável'\n",
      " 'redentora' 'refletindo' 'ressoa' 'reviravoltas' 'roteiro' 'ruim' 'sem'\n",
      " 'sentido' 'seu' 'seus' 'simplesmente' 'sonora' 'superficiais'\n",
      " 'surpreendeu' 'surpresas' 'são' 'te' 'tempo' 'terminar' 'terrível'\n",
      " 'tinham' 'tive' 'tocante' 'tocantes' 'todo' 'todos' 'total' 'totalmente'\n",
      " 'trama' 'trilha' 'trouxe' 'tudo' 'tão' 'um' 'uma' 'vale' 'verdadeira'\n",
      " 'vi' 'viciante' 'vida' 'visual' 'vou' 'vários']\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Testes, explorando vectorizer\n",
    "new_section(\"Testes, explorando vectorizer\")\n",
    "subsection(\"vectorizer.get_feature_names_out()[0]\")\n",
    "print(vectorizer.get_feature_names_out()[0])\n",
    "subsection(\"vectorizer.get_feature_names_out()[1]\")\n",
    "print(vectorizer.get_feature_names_out()[1])\n",
    "subsection(\"vectorizer.get_feature_names_out()[2]\")\n",
    "print(vectorizer.get_feature_names_out()[2])\n",
    "subsection(\"vectorizer.get_feature_names_out()\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "sep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================ Explorando vectorizer.transform() ============================================\n",
      "\n",
      "x = 'O filme é bom'------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "x_vec = vectorizer_corpus.transform([x])------------------------------------------------------------------------------------------------------------------------\n",
      "x_vec.shape :\n",
      "(1, 9)\n",
      "x_vec :\n",
      "  (0, 0)\t0.7898069290660905\n",
      "  (0, 1)\t0.6133555370249717\n",
      "type(x_vec) :\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "x_vec.todense() :\n",
      "[[0.78980693 0.61335554 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "binarizando x_vec.todense() :\n",
      "[[1 1 0 0 0 0 0 0 0]]\n",
      "************************************************************************************************************************\n",
      "\n",
      "y = 'O filme é muito ruim'------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "y_vec = vectorizer_corpus.transform([y])------------------------------------------------------------------------------------------------------------------------\n",
      "y_vec.shape :\n",
      "(1, 9)\n",
      "y_vec :\n",
      "  (0, 1)\t0.4532946552278861\n",
      "  (0, 3)\t0.4532946552278861\n",
      "  (0, 7)\t0.7674945674619879\n",
      "type(y_vec) :\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "y_vec.todense() :\n",
      "[[0.         0.45329466 0.         0.45329466 0.         0.\n",
      "  0.         0.76749457 0.        ]]\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"O filme é muito bom, recomendo a todos\",\n",
    "          \"O filme é muito bom, mas não recomendo a ninguém\",\n",
    "          \"O filme é muito ruim, mas recomendo a todos\"]\n",
    "vectorizer_corpus = TfidfVectorizer()\n",
    "X_vectorized_corpus = vectorizer_corpus.fit_transform(corpus, y=None)\n",
    "\n",
    "new_section(\"Explorando vectorizer.transform()\")\n",
    "subsection(\"x = 'O filme é bom'\")\n",
    "x = \"O filme é bom\"\n",
    "subsection(\"x_vec = vectorizer_corpus.transform([x])\")\n",
    "x_vec = vectorizer_corpus.transform([x])\n",
    "print(\"x_vec.shape :\")\n",
    "print(x_vec.shape)\n",
    "print(\"x_vec :\")\n",
    "print(x_vec)\n",
    "print(\"type(x_vec) :\")\n",
    "print(type(x_vec))\n",
    "print(\"x_vec.todense() :\")\n",
    "print(x_vec.todense())\n",
    "print(\"binarizando x_vec.todense() :\")\n",
    "print((x_vec.todense() > 0).astype(int))\n",
    "sep(sep=\"*\")\n",
    "subsection(\"y = 'O filme é muito ruim'\")\n",
    "y = \"O filme é muito ruim\"\n",
    "subsection(\"y_vec = vectorizer_corpus.transform([y])\")\n",
    "y_vec = vectorizer_corpus.transform([y])\n",
    "print(\"y_vec.shape :\")\n",
    "print(y_vec.shape)\n",
    "print(\"y_vec :\")\n",
    "print(y_vec)\n",
    "print(\"type(y_vec) :\")\n",
    "print(type(y_vec))\n",
    "print(\"y_vec.todense() :\")\n",
    "print(y_vec.todense())\n",
    "sep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 0 0]\n",
      "************************************************************************************************************************\n",
      "[1 1 0 0 0 0 0 0 0]\n",
      "************************************************************************************************************************\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def binarize(x): \n",
    "    x_bin = []\n",
    "    n = len(vectorizer_corpus.get_feature_names_out()) # Número de palavras no vetorizador\n",
    "    x_bin=np.zeros(n, dtype=int) # Vetor a ser binarizado\n",
    "    for i in x.indices:\n",
    "        x_bin[i] = 1 # Binarização, ativamos a palavra contida na sentença\n",
    "    return x_bin\n",
    "def binarize2(x): \n",
    "    x = (x.todense() > 0).astype(int)\n",
    "    return x.getA1()\n",
    "print(binarize(x_vec))\n",
    "print(\"*\"*120)\n",
    "print(binarize2(x_vec))\n",
    "print(\"*\"*120)\n",
    "print(type(binarize2(x_vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4054651099999997, 2.4054651099999997, 4.8109302199999995, 2.4054651099999997, 4, 2.4054651099999997, 4, 2.4054651099999997, 2.4054651099999997, 4.8109302199999995, 2.4054651099999997, 4, 2.4054651099999997, 4]\n"
     ]
    }
   ],
   "source": [
    "a = [1*(1.40546511+1), 1*(1.40546511+1), 2*(1.40546511+1), 1*(1.40546511+1), 2*(1+1), 1*(1.40546511+1), 2*(1+1)]\n",
    "a_array = np.array(a)\n",
    "a_22 = a_array@a_array\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4532946552278861\n",
      "  (0, 3)\t0.4532946552278861\n",
      "  (0, 7)\t0.7674945674619879\n",
      "weights :\n",
      "[0.45329466 0.45329466 0.76749457]\n",
      "weights_normalized :\n",
      "[0.27077177 0.27077177 0.45845646]\n",
      "[0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      "[0 1 0 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 6\n",
    "def samples(x):\n",
    "    n = len(vectorizer_corpus.get_feature_names_out())\n",
    "    x_indices = x.indices\n",
    "    n_x_words = len(x.indices)    \n",
    "    sample_set = [np.zeros(n) for i in range(num_samples-1)]\n",
    "    sample_set.append(binarize2(x))\n",
    "    weights = x.tocoo().data\n",
    "    print(\"weights :\")\n",
    "    print(weights)\n",
    "    weights_normalized = weights / weights.sum()\n",
    "    print(\"weights_normalized :\")\n",
    "    print(weights_normalized)\n",
    "    for i in range(num_samples-1):\n",
    "        num_words = np.random.randint(1, n_x_words+1)\n",
    "        z_line_indices = np.random.choice(x_indices, size=num_words, p=weights_normalized, replace=False)\n",
    "        sample_set[i][z_line_indices] = 1\n",
    "    return sample_set\n",
    "print(y_vec)\n",
    "sampless = samples(y_vec)\n",
    "for sample in sampless:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.39789669894933666\n",
      "  (0, 3)\t0.39789669894933666\n",
      "  (0, 0)\t0.5123644459248041\n",
      "  (0, 6)\t0.39789669894933666\n",
      "  (0, 8)\t0.5123644459248041\n",
      "  (1, 1)\t0.2880786492345198\n",
      "  (1, 3)\t0.2880786492345198\n",
      "  (1, 0)\t0.3709537120754161\n",
      "  (1, 6)\t0.2880786492345198\n",
      "  (1, 2)\t0.3709537120754161\n",
      "  (1, 5)\t0.4877595527309447\n",
      "  (1, 4)\t0.4877595527309447\n",
      "  (2, 1)\t0.3299953074645687\n",
      "  (2, 3)\t0.3299953074645687\n",
      "  (2, 6)\t0.3299953074645687\n",
      "  (2, 8)\t0.4249290414153384\n",
      "  (2, 2)\t0.4249290414153384\n",
      "  (2, 7)\t0.5587306244316468\n",
      "['bom' 'filme' 'mas' 'muito' 'ninguém' 'não' 'recomendo' 'ruim' 'todos']\n",
      "bom filme mas muito ninguém não recomendo ruim todos\n",
      "  (0, 0)\t0.3162910421180881\n",
      "  (0, 1)\t0.2456282096992013\n",
      "  (0, 2)\t0.3162910421180881\n",
      "  (0, 3)\t0.2456282096992013\n",
      "  (0, 4)\t0.4158847107181897\n",
      "  (0, 5)\t0.4158847107181897\n",
      "  (0, 6)\t0.2456282096992013\n",
      "  (0, 7)\t0.4158847107181897\n",
      "  (0, 8)\t0.3162910421180881\n",
      "[0.31629104 0.24562821 0.31629104 0.24562821 0.41588471 0.41588471\n",
      " 0.24562821 0.41588471 0.31629104]\n",
      "[0 1 2 3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "print(X_vectorized_corpus)\n",
    "lista_de_palavras = vectorizer_corpus.get_feature_names_out() \n",
    "print(lista_de_palavras)\n",
    "super_frase = \" \".join(lista_de_palavras)\n",
    "print(super_frase)\n",
    "super_frase = vectorizer_corpus.transform([super_frase])\n",
    "print(super_frase)\n",
    "print(super_frase.tocoo().data)   \n",
    "print(super_frase.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4532946552278861\n",
      "  (0, 3)\t0.4532946552278861\n",
      "  (0, 7)\t0.7674945674619879\n",
      "************************************************************************************************************************\n",
      "  (0, 7)\t1.0\n",
      "************************************************************************************************************************\n",
      "  (0, 3)\t0.5085423203783267\n",
      "  (0, 7)\t0.8610369959439764\n",
      "************************************************************************************************************************\n",
      "  (0, 3)\t1.0\n",
      "************************************************************************************************************************\n",
      "  (0, 1)\t0.4532946552278861\n",
      "  (0, 3)\t0.4532946552278861\n",
      "  (0, 7)\t0.7674945674619879\n",
      "************************************************************************************************************************\n",
      "  (0, 1)\t0.4532946552278861\n",
      "  (0, 3)\t0.4532946552278861\n",
      "  (0, 7)\t0.7674945674619879\n",
      "************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def sentences_samples(z_line):\n",
    "    indices = np.where(z_line == 1)[0]\n",
    "    z=\" \".join([vectorizer_corpus.get_feature_names_out()[indice] for indice in indices])\n",
    "    return vectorizer_corpus.transform([z])\n",
    "for i in sampless:\n",
    "    print(sentences_samples(i))\n",
    "    print(\"*\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================== Teste de cosine_similarity ===============================================\n",
      "  (0, 3)\t0.4532946552278861\n",
      "  (0, 5)\t0.7674945674619879\n",
      "  (0, 6)\t0.4532946552278861\n",
      "----------\n",
      "  (0, 0)\t0.512364445924804\n",
      "  (0, 1)\t0.3978966989493366\n",
      "  (0, 2)\t0.512364445924804\n",
      "  (0, 3)\t0.3978966989493366\n",
      "  (0, 6)\t0.3978966989493366\n",
      "\n",
      "cosine_similarity(x_vec, y_vec)------------------------------------------------------------------------------------------------------------------------\n",
      "[[0.36072889]]\n",
      "\n",
      "cosine_similarity_my(X_vectorized[instance_index_neg], X_vectorized[instance_index_pos])------------------------------------------------------------------------------------------------------------------------\n",
      "0.3607288939331071\n",
      "\n",
      "kernel(x_vec, y_vec)------------------------------------------------------------------------------------------------------------------------\n",
      "0.19660341787972543\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "new_section(\"Teste de cosine_similarity\")\n",
    "def cosine_similarity_my(x, z):\n",
    "    x = x.todense()\n",
    "    z = z.todense()\n",
    "    dot_product = x.dot(z.T)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_z = np.linalg.norm(z)\n",
    "    return (dot_product / (norm_x * norm_z)).getA1()[0]\n",
    "def kernel(x, z):\n",
    "    distance = cosine_similarity_my(x, z) # Similaridade de cosseno\n",
    "    weight = np.sqrt(np.exp(-(distance**2) / (0.2**2)))  # Kernel exponencial\n",
    "    return weight\n",
    "x_vec = vectorizer_corpus.transform([\"não recomendo muito\"])\n",
    "y_vec = vectorizer_corpus.transform([\"nignuém mas filme bom, recomendo muito\"])\n",
    "print(x_vec)\n",
    "print(\"----------\")\n",
    "print(y_vec)\n",
    "subsection(\"cosine_similarity(x_vec, y_vec)\")\n",
    "print(cosine_similarity(x_vec, y_vec))\n",
    "subsection(\"cosine_similarity_my(X_vectorized[instance_index_neg], X_vectorized[instance_index_pos])\")\n",
    "print(cosine_similarity_my(x_vec,y_vec))\n",
    "subsection(\"kernel(x_vec, y_vec)\")\n",
    "print(kernel(x_vec, y_vec))\n",
    "\n",
    "sep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime para explicação de texto    \n",
    "class LimeExplainerSentences:\n",
    "    def __init__(self, sigma=25**2, num_samples=15000, K=6, alpha=0.1**(16), p=16,vectorizer=None, model=None, seed=42, generate=\"opt\"):\n",
    "        self.sigma = sigma # Parâmetro do kernel exponencial\n",
    "        self.num_samples = num_samples # Número de amostras geradas \n",
    "        self.K = K # Número de palavras importantes\n",
    "        self.alpha = alpha # Parâmetro de regularização Lasso\n",
    "        self.model = model # Modelo de classificação\n",
    "        self.vectorizer = vectorizer # Vetorizador\n",
    "        self.generate = generate # Tipo de amostragem\n",
    "        self.p = p # Precisão de impressão\n",
    "        np.random.seed(seed) # Semente aleatória\n",
    "        np.set_printoptions(precision=self.p) # Precisão de impressão\n",
    "        \n",
    "\n",
    "    # Binarizar vetor de palavras\n",
    "    def binarize(self, x): \n",
    "        x = (x.todense() > 0).astype(int) # Binariza o vetor\n",
    "        return x.getA1() # Retorna o vetor em formato de array\n",
    "\n",
    "    def cossine_similarity(self, x, z):\n",
    "        x = x.todense() # Transforma o vetor em uma matriz densa\n",
    "        z = z.todense()\n",
    "        dot_product = x.dot(z.T)\n",
    "        norm_x = np.linalg.norm(x)\n",
    "        norm_z = np.linalg.norm(z)\n",
    "        return (dot_product / (norm_x * norm_z)).getA1()[0]\n",
    "    \n",
    "    # Define a função de kernel \n",
    "    def kernel(self, x, z):\n",
    "        distance = self.cossine_similarity(x, z) # Similaridade de cosseno\n",
    "        weight = np.sqrt(np.exp(-(distance**2) / (self.sigma**2)))  # Kernel exponencial\n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def samples_simples(self, x):\n",
    "        n = len(self.vectorizer.get_feature_names_out()) # Número de palavras no vetorizador\n",
    "        x_indices = x.indices # Índices das palavras na sentença\n",
    "        n_x_words = len(x.indices) # Número de palavras na sentença   \n",
    "        sample_set = [np.zeros(n) for i in range(self.num_samples-1)] # Conjunto de amostras\n",
    "        sample_set.append(self.binarize(x)) # Adiciona a sentença original binarizada\n",
    "        Z_line_indices = [] # Índices das palavras ativadas\n",
    "        for i in range(self.num_samples-1): # Gerar amostras aleatórias\n",
    "            num_words = np.random.randint(1, n_x_words+1)  # Número de palavras na amostra\n",
    "            # Escolhe aleatoriamente as palavras da sentença original\n",
    "            # size = número de palavras na amostra\n",
    "            # replace = False, não permite repetição\n",
    "            z_line_indices = np.random.choice(x_indices, size=num_words, replace=False)\n",
    "            # Ativa as palavras escolhidas\n",
    "            sample_set[i][z_line_indices] = 1\n",
    "            Z_line_indices.append(z_line_indices)\n",
    "        return sample_set, z_line_indices\n",
    "\n",
    "    # Gera dados ao redor de x_line relevancia das palavras\n",
    "    def samples_opt(self, x):\n",
    "        n = len(self.vectorizer.get_feature_names_out()) # Número de palavras no vetorizador\n",
    "        x_indices = x.indices # Índices das palavras na sentença\n",
    "        n_x_words = len(x.indices) # Número de palavras na sentença   \n",
    "        sample_set = [np.zeros(n) for i in range(self.num_samples-1)] # Conjunto de amostras\n",
    "        sample_set.append(self.binarize(x)) # Adiciona a sentença original binarizada\n",
    "        weights = x.tocoo().data # Pesos das palavras\n",
    "        weights_normalized = weights / weights.sum() # Normaliza os pesos\n",
    "        Z_line_indices = [] # Índices das palavras ativadas\n",
    "        for i in range(self.num_samples-1): # Gerar amostras aleatórias\n",
    "            num_words = np.random.randint(1, n_x_words+1)  # Número de palavras na amostra\n",
    "            # Escolhe aleatoriamente as palavras da sentença original\n",
    "            # size = número de palavras na amostra\n",
    "            # p = pesos normalizados, a probabilidade de escolher cada palavra\n",
    "            # replace = False, não permite repetição\n",
    "            z_line_indices = np.random.choice(x_indices, size=num_words, p=weights_normalized, replace=False)\n",
    "            # Ativa as palavras escolhidas\n",
    "            sample_set[i][z_line_indices] = 1\n",
    "            Z_line_indices.append(z_line_indices)\n",
    "        return sample_set, z_line_indices\n",
    "    \n",
    "    \n",
    "    # Transforma um vetor em uma frase\n",
    "    def sentences_samples(self, Z_line):\n",
    "        for z_line in Z_line:\n",
    "            z=\" \".join([self.vectorizer.get_feature_names_out()[indice] for indice in Z_line])\n",
    "        return self.vectorizer.transform([z])\n",
    "\n",
    "    # Define o vetor de pesos\n",
    "    def LIME(self, x):\n",
    "        if self.generate == \"opt\":\n",
    "            Z_line = self.samples_opt\n",
    "        else:\n",
    "            Z_line = self.samples_simples\n",
    "        Z=[]\n",
    "        for i in range(len(Z_line)):\n",
    "            Z.append(self.sentences_samples(Z_line[i]))\n",
    "        Z_pred = np.array([self.model.predict(z)[0] for z in Z])  \n",
    "        pi_x = np.array([self.kernel(x, z)[0][0] for z in Z]) \n",
    "        lasso = Lasso(alpha=self.alpha)\n",
    "        lasso.fit(Z_line, Z_pred, sample_weight=pi_x)\n",
    "        w = lasso.coef_\n",
    "        return w \n",
    "\n",
    "    # Gerar explicação\n",
    "    def explain_instance(self, x):\n",
    "        w = self.LIME(x)\n",
    "        abs_valores = np.abs(w)\n",
    "        indices = np.argsort(abs_valores)[::-1][:self.K]\n",
    "        print(\"Palavras importantes:\")\n",
    "        for i in indices:\n",
    "            print(f\"{self.vectorizer.get_feature_names_out()[i]}: {w[i]}\")    \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubmodularPick:\n",
    "    def __init__(self,X_n_vec, X, B,lime=None, vectorizer=None):\n",
    "        self.X_n_vec = X_n_vec\n",
    "        self.X = X\n",
    "        self.B = B\n",
    "        self.lime = lime\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def explain_instances(self, X):\n",
    "        W = [self.lime.LIME(x) for x in X] \n",
    "        return np.array(W)      \n",
    "        \n",
    "    def importancia(self, W):\n",
    "        I = np.zeros(W.shape[1])  \n",
    "        for j in range(W.shape[1]):\n",
    "            soma = np.sum(np.abs(W[:, j]))\n",
    "            I[j] = np.sqrt(soma)\n",
    "        return I\n",
    "\n",
    "    def cobertura(self, V, W, I):\n",
    "        c_value = 0  \n",
    "        for j in range(W.shape[1]):\n",
    "            if any(W[i, j] > 0 for i in V):  # Se a característica j é relevante\n",
    "                c_value += I[j]\n",
    "        return c_value\n",
    "\n",
    "    def guloso(self, X, B):\n",
    "        W = self.explain_instances(X) \n",
    "        I = self.importancia(W)\n",
    "        nao_selecionados = list(range(W.shape[0]))  # Lista de índices não selecionados\n",
    "        V = []  # Conjunto de características selecionadas\n",
    "        itens = 0  # Número de elementos selecionados\n",
    "        c_value = 0  # Valor de c\n",
    "\n",
    "        while itens < B and nao_selecionados:\n",
    "            best_gain = -np.inf  # Valor de ganho máximo\n",
    "            best_item = None  # Índice do melhor item \n",
    "\n",
    "            for item in nao_selecionados:  # Itera sobre os itens não selecionados\n",
    "                lista_temp = V + [item]\n",
    "                gain = self.cobertura(lista_temp, W, I) - self.cobertura(V, W, I)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain  # Atualiza o valor de ganho máximo\n",
    "                    best_item = item  # Atualiza o melhor item\n",
    "\n",
    "            if best_item is not None:\n",
    "                V.append(best_item)  # Adiciona o melhor item ao conjunto\n",
    "                nao_selecionados.remove(best_item)  # Remove o melhor item dos não selecionados\n",
    "                itens += 1\n",
    "                c_value += best_gain\n",
    "\n",
    "        return V\n",
    "    \n",
    "    def explain_model(self, X):\n",
    "        V = self.guloso(X, self.B)\n",
    "        print(\"Melhor conjunto de explicação: \", V)\n",
    "        for i in V:\n",
    "            print(f\"{self.X_n_vec[i]}\")\n",
    "        print(\"Fim da explicação\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
