{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Isas_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import Lasso\n",
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Explicador LIME\n",
    "import lime\n",
    "import lime.lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instância: Nunca vi algo tão ruim na minha vida. A produção foi amadora e a história, entediante.\n",
      "Rótulo: 0\n",
      "[array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), array([0, 1, 1, 1, 1, 1, 0, 1, 0, 1]), array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de conjunto de dados de resenhas\n",
    "dados = pd.read_csv('dados.csv', sep=';')\n",
    "\n",
    "# Separar as features e o alvo\n",
    "X = dados['review']\n",
    "y = dados['sentimentos']\n",
    "\n",
    "# Escolher uma instância para fazer a previsão\n",
    "instance_index = 15\n",
    "instance = X.iloc[instance_index]\n",
    "instance_label = y.iloc[instance_index]\n",
    "print('Instância:', instance)\n",
    "print('Rótulo:', instance_label)\n",
    "print([np.random.randint(2, size = 10) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirar stop-words\n",
    "stop_words = stopwords.words('portuguese')\n",
    "X_sw = X.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Vetorização do texto\n",
    "vectorizer_sw = TfidfVectorizer()\n",
    "X_vectorized_sw = vectorizer_sw.fit_transform(X_sw)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train_sw, X_test_sw, y_train, y_test = train_test_split(X_vectorized_sw, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 125)\t0.33409972762024637\n",
      "  (0, 182)\t0.4023535870696947\n",
      "  (0, 248)\t0.3876307185009198\n",
      "  (0, 40)\t0.4023535870696947\n",
      "  (0, 219)\t0.2711927452342461\n",
      "  (0, 124)\t0.3259287506501664\n",
      "  (0, 175)\t0.4044076630246902\n",
      "  (0, 163)\t0.266500218652219\n",
      "[(0.33409972762024637,), (0.4023535870696947,), (0.3876307185009198,), (0.4023535870696947,), (0.2711927452342461,), (0.3259287506501664,), (0.4044076630246902,), (0.266500218652219,)]\n",
      "[0.3340997276202464 0.4023535870696947 0.3876307185009198\n",
      " 0.4023535870696947 0.2711927452342461 0.3259287506501664\n",
      " 0.4044076630246902 0.266500218652219 ]\n",
      "!!!!!!!!!!!!!!!!!!!!\n",
      "foi\n",
      "  (0, 125)\t0.33409972762024637\n",
      "  (0, 182)\t0.4023535870696947\n",
      "  (0, 248)\t0.3876307185009198\n",
      "  (0, 40)\t0.4023535870696947\n",
      "  (0, 219)\t0.2711927452342461\n",
      "  (0, 124)\t0.3259287506501664\n",
      "  (0, 175)\t0.4044076630246902\n",
      "  (0, 163)\t0.266500218652219\n",
      "não\n",
      "(98,)\n",
      "(98, 256)\n"
     ]
    }
   ],
   "source": [
    "# Deixar stopwords\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "print(X_vectorized[59].tocoo())\n",
    "lista =[]\n",
    "for data in zip(X_vectorized[59].tocoo().data):\n",
    "    lista.append(data)\n",
    "print(lista)\n",
    "print(X_vectorized[59].tocoo().data)\n",
    "print(\"!\"*20)\n",
    "#print(vectorizer.get_feature_names_out())\n",
    "print(vectorizer.get_feature_names_out()[118])\n",
    "#print(vectorizer.get_feature_names_out()[54])\n",
    "#print(vectorizer.get_feature_names_out()[246])\n",
    "#print(vectorizer.get_feature_names_out()[65])\n",
    "#print(vectorizer.get_feature_names_out()[239])\n",
    "#print(vectorizer.get_feature_names_out()[230])\n",
    "#print(vectorizer.get_feature_names_out()[217])\n",
    "#print(\"-\"*20)\n",
    "#print(X_vectorized[1][0])\n",
    "#print(X[1])\n",
    "print((X_vectorized[59]))\n",
    "#for i,j in X_vectorized[59].items():\n",
    "#    print(i,j)\n",
    "print(vectorizer.get_feature_names_out()[175])\n",
    "print(X.shape)\n",
    "print(X_vectorized.shape)\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento 0:\n",
      "  data: 0.5051001005334584\n",
      "  text: 0.4089220628888078\n",
      "  of: 0.6406554311067799\n",
      "  example: 0.4089220628888078\n",
      "Documento 1:\n",
      "  another: 0.6578293132998527\n",
      "  text: 0.5325695234255544\n",
      "  example: 0.5325695234255544\n",
      "Documento 2:\n",
      "  testing: 0.4981971092712959\n",
      "  for: 0.4981971092712959\n",
      "  more: 0.4981971092712959\n",
      "  data: 0.3927843233041577\n",
      "  text: 0.3179927614076589\n",
      "Documento 3:\n",
      "  documents: 0.40726515033596056\n",
      "  enough: 0.40726515033596056\n",
      "  ensure: 0.40726515033596056\n",
      "  to: 0.40726515033596056\n",
      "  yet: 0.40726515033596056\n",
      "  another: 0.32109252242362013\n",
      "  example: 0.2599520699143871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Amostra de dados de texto\n",
    "documents = [\n",
    "    \"example of text data\",\n",
    "    \"another text example\",\n",
    "    \"more text data for testing\",\n",
    "    \"yet another example to ensure enough documents\"\n",
    "]\n",
    "\n",
    "# Criando e aplicando o TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Função para obter a lista de pesos TF-IDF\n",
    "def get_tfidf_weights(vectorized_matrix, feature_names):\n",
    "    # Convertendo a matriz esparsa para formato COO para acesso fácil\n",
    "    coo_matrix = vectorized_matrix.tocoo()\n",
    "    \n",
    "    # Criando um dicionário para armazenar os pesos TF-IDF para cada termo em cada documento\n",
    "    tfidf_weights = {}\n",
    "    \n",
    "    # Iterando sobre os dados da matriz COO\n",
    "    for row, col, data in zip(coo_matrix.row, coo_matrix.col, coo_matrix.data):\n",
    "        term = feature_names[col]\n",
    "        if row not in tfidf_weights:\n",
    "            tfidf_weights[row] = {}\n",
    "        tfidf_weights[row][term] = data\n",
    "    \n",
    "    return tfidf_weights\n",
    "\n",
    "# Obtendo os nomes das features (termos)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Chamando a função para obter os pesos TF-IDF\n",
    "tfidf_weights = get_tfidf_weights(X_vectorized, feature_names)\n",
    "\n",
    "# Imprimindo os pesos TF-IDF para cada documento\n",
    "for doc_index, weights in tfidf_weights.items():\n",
    "    print(f\"Documento {doc_index}:\")\n",
    "    for term, weight in weights.items():\n",
    "        print(f\"  {term}: {weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Confirmando que você tem pelo menos 60 entradas (o que obviamente não acontece aqui, então vamos adaptar para o tamanho da lista)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_vectorized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Alterado para verificar pelo menos um documento\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Chamando a função para a linha 1 (índice 0)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     second_column_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_second_column_data_from_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_vectorized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Imprimindo o resultado\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÍndice fora do alcance.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[124], line 17\u001b[0m, in \u001b[0;36mget_second_column_data_from_row\u001b[1;34m(X_row)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mEsta função extrai todos os elementos não-nulos da segunda coluna de uma única entrada TF-IDF.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m- Uma lista de tuplas, cada tupla contendo (índice da linha, valor TF-IDF) para a segunda coluna.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Acessa a segunda coluna da entrada\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_row\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè o que to pensando?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def get_second_column_data_from_row(X_row):\n",
    "    \"\"\"\n",
    "    Esta função extrai todos os elementos não-nulos da segunda coluna de uma única entrada TF-IDF.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - X_row: Uma única linha de uma matriz scipy.sparse.csr_matrix, matriz TF-IDF de um documento.\n",
    "    \n",
    "    Retorna:\n",
    "    - Uma lista de tuplas, cada tupla contendo (índice da linha, valor TF-IDF) para a segunda coluna.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Acessa a segunda coluna da entrada\n",
    "    print(X_row.getcol(2).tocoo().data[0])\n",
    "    print(\"è o que to pensando?\")\n",
    "    #second_col = X_row.getcol(1).tocoo().data  # Transforma a segunda coluna em formato COO\n",
    "    \n",
    "    # Extrair índices e valores\n",
    "    #second_col_data = list(zip(second_col.row, second_col.data))\n",
    "    \n",
    "    #return second_col_data\n",
    "\n",
    "# Dados de exemplo, certificando-se de que são todas strings válidas\n",
    "documents = [\n",
    "    \"example of text data\",\n",
    "    \"another text example\",\n",
    "    \"more text data for testing\",\n",
    "    \"yet another example to ensure enough documents\"\n",
    "]\n",
    "\n",
    "# Criando e aplicando o TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Confirmando que você tem pelo menos 60 entradas (o que obviamente não acontece aqui, então vamos adaptar para o tamanho da lista)\n",
    "if X_vectorized.shape[0] > 0:  # Alterado para verificar pelo menos um documento\n",
    "    # Chamando a função para a linha 1 (índice 0)\n",
    "    second_column_data = get_second_column_data_from_row(X_vectorized[0])\n",
    "\n",
    "    # Imprimindo o resultado\n",
    "\n",
    "else:\n",
    "    print(\"Índice fora do alcance.\")\n",
    "print(X_vectorized[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index: 1, Feature name: data, TF-IDF Value: 0.4804583972923858\n",
      "Feature index: 5, Feature name: text, TF-IDF Value: 0.4804583972923858\n",
      "Feature index: 4, Feature name: of, TF-IDF Value: 0.6317450542765208\n",
      "Feature index: 2, Feature name: example, TF-IDF Value: 0.3731188059313277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = [\"example of text data\", \"another example\", \"more text data example\"]\n",
    "\n",
    "# Create the vectorizer and transform the data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Access the first document vector (first row of the matrix)\n",
    "first_doc_vector = X_vectorized[0]\n",
    "\n",
    "# Print the features and their TF-IDF scores\n",
    "# Convert to COO format for easy iteration over nonzero elements\n",
    "first_doc_vector_coo = first_doc_vector.tocoo()\n",
    "\n",
    "# Iterate over each feature index and its corresponding value\n",
    "for idx, value in zip(first_doc_vector_coo.col, first_doc_vector_coo.data):\n",
    "    print(f\"Feature index: {idx}, Feature name: {vectorizer.get_feature_names_out()[idx]}, TF-IDF Value: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importancia(W: np.array) -> np.array:\n",
    "    I = np.zeros(W.shape[1])  # Vetor de importância\n",
    "    for j in range(W.shape[1]):\n",
    "        soma = np.sum(np.abs(W[:, j]))\n",
    "        I[j] = np.sqrt(soma)\n",
    "    return I\n",
    "\n",
    "def c(V: list, W: np.array, I: np.array) -> float:\n",
    "    c_value = 0  # Valor de c\n",
    "    for j in range(W.shape[1]):\n",
    "        if any(W[i, j] > 0 for i in V):  # Se a característica j é relevante\n",
    "            c_value += I[j]\n",
    "    return c_value\n",
    "\n",
    "def guloso(W: np.array, I: np.array, B: int) -> list:\n",
    "    nao_selecionados = list(range(W.shape[0]))  # Lista de índices não selecionados\n",
    "    V = []  # Conjunto de características selecionadas\n",
    "    itens = 0  # Número de elementos selecionados\n",
    "    c_value = 0  # Valor de c\n",
    "    \n",
    "    while itens < B and nao_selecionados:\n",
    "        best_gain = -np.inf  # Valor de ganho máximo\n",
    "        best_item = None  # Índice do melhor item \n",
    "        \n",
    "        for item in nao_selecionados:  # Itera sobre os itens não selecionados\n",
    "            lista_temp = V + [item]\n",
    "            gain = c(lista_temp, W, I) - c(V, W, I)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain  # Atualiza o valor de ganho máximo\n",
    "                best_item = item  # Atualiza o melhor item\n",
    "        \n",
    "        if best_item is not None:\n",
    "            V.append(best_item)  # Adiciona o melhor item ao conjunto\n",
    "            nao_selecionados.remove(best_item)  # Remove o melhor item dos não selecionados\n",
    "            itens += 1\n",
    "            c_value += best_gain\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de importância:  [2.  2.  2.2]\n",
      "Cobertura:  6.23606797749979\n",
      "Melhor conjunto:  [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "W = np.array([[1, 2, 1], [-1, 0, 2], [-2, 2, -2]])  # Exemplo de matriz W\n",
    "I = importancia(W)\n",
    "print(\"Matriz de importância: \", I)\n",
    "\n",
    "V = [0, 1]  # Lista de índices das instâncias selecionadas\n",
    "print(\"Cobertura: \", c(V, W, I))\n",
    "\n",
    "V_linha = guloso(W, I, 2)\n",
    "print(\"Melhor conjunto: \", V_linha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.zeros(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não gostei. Muito chato e sem graça. Não vale a pena.\n",
      "chato gostei graça muito não pena sem vale\n"
     ]
    }
   ],
   "source": [
    "x = X_vectorized[59]\n",
    "print(X[59])\n",
    "def binarize(x):\n",
    "    n = len(vectorizer.get_feature_names_out())\n",
    "    x_bin=np.zeros(n, dtype=int)\n",
    "    for i in x.indices:\n",
    "        x_bin[i] = 1\n",
    "    return x_bin\n",
    "num_samples = 5\n",
    "n = len(vectorizer.get_feature_names_out())\n",
    "x_indices = x.indices\n",
    "n_x_words = len(x.indices)    \n",
    "sample_set = [np.zeros(n) for i in range(num_samples-1)]\n",
    "sample_set.append(binarize(x))\n",
    "for i in range(num_samples-1):\n",
    "    z_line_indices = np.random.randint(2, size=n_x_words)\n",
    "    while not np.any(z_line_indices):  \n",
    "        z_line_indices = np.random.randint(2, size=n_x_words)\n",
    "    z_line_indices = np.where(z_line_indices == 1)[0]\n",
    "    activated_words = [x_indices[j] for j in z_line_indices]\n",
    "    sample_set[i][activated_words] = 1\n",
    "\n",
    "indices = np.where(binarize(x)== 1)[0]\n",
    "z=\" \".join([vectorizer.get_feature_names_out()[indice] for indice in indices])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lista \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m lista[[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "lista = np.array[0,0,0,0,0]\n",
    "lista[[1,2]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LimeExplainerSentences:\n",
    "    def __init__(self, sigma=0.2, num_samples=1000, K=5, alpha=0.1**(-5), p =10,vectorizer=None, model=None):\n",
    "        self.sigma = sigma\n",
    "        self.num_samples = num_samples\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.p = p\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "    \n",
    "    # Binarizar vetor de palavras\n",
    "    def binarize(self, x):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        n = len(self.vectorizer.get_feature_names_out())\n",
    "        x_bin=np.zeros(n, dtype=int)\n",
    "        for i in x.indices:\n",
    "            x_bin[i] = 1\n",
    "        return x_bin\n",
    "\n",
    "    # Define a função de kernel \n",
    "    def kernel(self, x, z):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        distance = cosine_similarity(x, z) # Similaridade de cosseno\n",
    "        pi_x = np.sqrt(np.exp(-(distance**2) / (self.sigma**2)))  # Kernel \n",
    "        return pi_x\n",
    "        \n",
    "    # Gera dados ao redor de x_line\n",
    "    def samples(self, x):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        n = len(self.vectorizer.get_feature_names_out())\n",
    "        x_indices = x.indices\n",
    "        n_x_words = len(x.indices)    \n",
    "        sample_set = [np.zeros(n) for i in range(self.num_samples-1)]\n",
    "        sample_set.append(self.binarize(x))\n",
    "        weights = x.tocoo().data\n",
    "        weights_normalized = weights / weights.sum()\n",
    "        for i in range(self.num_samples-1):\n",
    "            num_words = np.random.randint(1, n_x_words+1)\n",
    "            z_line_indices = np.random.choice(x_indices, size=num_words, p=weights_normalized, replace=False)\n",
    "            sample_set[i][z_line_indices] = 1\n",
    "        return sample_set\n",
    "    \n",
    "    # Transforma um vetor em uma frase\n",
    "    def sentences_samples(self, z_line):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        indices = np.where(z_line == 1)[0]\n",
    "        z=\" \".join([self.vectorizer.get_feature_names_out()[indice] for indice in indices])\n",
    "        return self.vectorizer.transform([z])\n",
    "\n",
    "    # Define o vetor de pesos\n",
    "    def LIME(self, x):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        Z_line = self.samples(x)\n",
    "        Z=[]\n",
    "        for i in range(len(Z_line)):\n",
    "            Z.append(self.sentences_samples(Z_line[i]))\n",
    "        Z_pred = np.array([self.model.predict(z)[0] for z in Z])  \n",
    "        pi_x = np.array([self.kernel(x, z)[0][0] for z in Z]) \n",
    "        lasso = Lasso(alpha=self.alpha)\n",
    "        lasso.fit(Z_line, Z_pred, sample_weight=pi_x)\n",
    "        w = lasso.coef_\n",
    "        return w \n",
    "\n",
    "    # Gerar explicação\n",
    "    def explain_instance(self, x):\n",
    "        w = self.LIME(x)\n",
    "        abs_valores = np.abs(w)\n",
    "        indices = np.argsort(abs_valores)[::-1][:self.K]\n",
    "        print(\"Palavras importantes:\")\n",
    "        for i in indices:\n",
    "            print(f\"{self.vectorizer.get_feature_names_out()[i]}: {w[i]}\")    \n",
    "\n",
    "\n",
    "class SubmodularPick:\n",
    "    def __init__(self,X_n_vec, X, B,lime=None, vectorizer=None):\n",
    "        self.X_n_vec = X_n_vec\n",
    "        self.X = X\n",
    "        self.B = B\n",
    "        self.lime = lime\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def explain_instances(self, X):\n",
    "        W = [self.lime.LIME(x) for x in X] \n",
    "        return np.array(W)      \n",
    "        \n",
    "    def importancia(self, W):\n",
    "        I = np.zeros(W.shape[1])  \n",
    "        for j in range(W.shape[1]):\n",
    "            soma = np.sum(np.abs(W[:, j]))\n",
    "            I[j] = np.sqrt(soma)\n",
    "        return I\n",
    "\n",
    "    def cobertura(self, V, W, I):\n",
    "        c_value = 0  \n",
    "        for j in range(W.shape[1]):\n",
    "            if any(W[i, j] > 0 for i in V):  # Se a característica j é relevante\n",
    "                c_value += I[j]\n",
    "        return c_value\n",
    "\n",
    "    def guloso(self, X, B):\n",
    "        W = self.explain_instances(X) \n",
    "        I = self.importancia(W)\n",
    "        nao_selecionados = list(range(W.shape[0]))  # Lista de índices não selecionados\n",
    "        V = []  # Conjunto de características selecionadas\n",
    "        itens = 0  # Número de elementos selecionados\n",
    "        c_value = 0  # Valor de c\n",
    "\n",
    "        while itens < B and nao_selecionados:\n",
    "            best_gain = -np.inf  # Valor de ganho máximo\n",
    "            best_item = None  # Índice do melhor item \n",
    "\n",
    "            for item in nao_selecionados:  # Itera sobre os itens não selecionados\n",
    "                lista_temp = V + [item]\n",
    "                gain = self.cobertura(lista_temp, W, I) - self.cobertura(V, W, I)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain  # Atualiza o valor de ganho máximo\n",
    "                    best_item = item  # Atualiza o melhor item\n",
    "\n",
    "            if best_item is not None:\n",
    "                V.append(best_item)  # Adiciona o melhor item ao conjunto\n",
    "                nao_selecionados.remove(best_item)  # Remove o melhor item dos não selecionados\n",
    "                itens += 1\n",
    "                c_value += best_gain\n",
    "\n",
    "        return V\n",
    "    \n",
    "    def explain_model(self, X):\n",
    "        V = self.guloso(X, self.B)\n",
    "        print(\"Melhor conjunto de explicação: \", V)\n",
    "        for i in V:\n",
    "            print(f\"{self.X_n_vec[i]}\")\n",
    "        print(\"Fim da explicação\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instância:  [ 80   7 179 223 216   5 221 155 196 114]\n",
      "Instância:  Nunca vi algo tão ruim na minha vida. A produção foi amadora e a história, entediante.\n",
      "Rótulo:  0\n",
      "Palavras importantes:\n",
      "produção: -0.03595303930759655\n",
      "amadora: -0.034949890325061905\n",
      "nunca: -0.03403948758597676\n"
     ]
    }
   ],
   "source": [
    "# Retirar stop-words\n",
    "stop_words = stopwords.words('portuguese')\n",
    "X_sw = X.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Vetorização do texto\n",
    "vectorizer_sw = TfidfVectorizer()\n",
    "X_vectorized_sw = vectorizer_sw.fit_transform(X_sw)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train_sw, X_test_sw, y_train, y_test = train_test_split(X_vectorized_sw, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinamento do modelo de Gradient Boosting\n",
    "gb_model_sw = GradientBoostingClassifier(random_state=42)\n",
    "gb_model_sw.fit(X_train_sw, y_train)\n",
    "# Separar as features e o alvo\n",
    "instance_index = 15\n",
    "\n",
    "x = X_vectorized_sw[instance_index]\n",
    "print(\"Instância: \", x.indices)\n",
    "print(\"Instância: \", instance)\n",
    "print(\"Rótulo: \", instance_label)\n",
    "\n",
    "LIME = LimeExplainerSentences(sigma=0.1, num_samples=15000, K=10, alpha=0.1**(12), p=16, vectorizer=vectorizer_sw, model=gb_model_sw)\n",
    "LIME.explain_instance(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instância:  [ 80   7 179 223 216   5 221 155 196 114]\n",
      "Instância:  Nunca vi algo tão ruim na minha vida. A produção foi amadora e a história, entediante.\n",
      "Rótulo:  0\n",
      "Palavras importantes:\n",
      "produção: -0.028800721662100955\n",
      "nunca: -0.02804175868154024\n",
      "amadora: -0.024345968198147355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Retirar stop-words\n",
    "stop_words = stopwords.words('portuguese')\n",
    "X_sw = X.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Vetorização do texto\n",
    "vectorizer_sw = TfidfVectorizer()\n",
    "X_vectorized_sw = vectorizer_sw.fit_transform(X_sw)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train_sw, X_test_sw, y_train, y_test = train_test_split(X_vectorized_sw, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinamento do modelo de Gradient Boosting\n",
    "gb_model_sw = GradientBoostingClassifier(random_state=42)\n",
    "gb_model_sw.fit(X_train_sw, y_train)\n",
    "# Separar as features e o alvo\n",
    "instance_index = 15\n",
    "\n",
    "x = X_vectorized_sw[instance_index]\n",
    "print(\"Instância: \", x.indices)\n",
    "print(\"Instância: \", instance)\n",
    "print(\"Rótulo: \", instance_label)\n",
    "\n",
    "LIME = LimeExplainerSentences(sigma=0.1, num_samples=5000, K=3, alpha=0.1**(5), p=1, vectorizer=vectorizer_sw, model=gb_model_sw)\n",
    "LIME.explain_instance(x)\n",
    "\n",
    "submodular = SubmodularPick(X_sw, X_vectorized_sw, 10, lime=LIME, vectorizer=vectorizer_sw)\n",
    "submodular.explain_model(X_vectorized_sw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
