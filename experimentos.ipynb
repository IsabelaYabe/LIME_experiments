{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importancia(W: np.array) -> np.array:\n",
    "    I = np.zeros(W.shape[1])  # Vetor de importância\n",
    "    for j in range(W.shape[1]):\n",
    "        soma = np.sum(np.abs(W[:, j]))\n",
    "        I[j] = np.sqrt(soma)\n",
    "    return I\n",
    "\n",
    "def c(V: list, W: np.array, I: np.array) -> float:\n",
    "    c_value = 0  # Valor de c\n",
    "    for j in range(W.shape[1]):\n",
    "        if any(W[i, j] > 0 for i in V):  # Se a característica j é relevante\n",
    "            c_value += I[j]\n",
    "    return c_value\n",
    "\n",
    "def guloso(W: np.array, I: np.array, B: int) -> list:\n",
    "    nao_selecionados = list(range(W.shape[0]))  # Lista de índices não selecionados\n",
    "    V = []  # Conjunto de características selecionadas\n",
    "    itens = 0  # Número de elementos selecionados\n",
    "    c_value = 0  # Valor de c\n",
    "    \n",
    "    while itens < B and nao_selecionados:\n",
    "        best_gain = -np.inf  # Valor de ganho máximo\n",
    "        best_item = None  # Índice do melhor item \n",
    "        \n",
    "        for item in nao_selecionados:  # Itera sobre os itens não selecionados\n",
    "            lista_temp = V + [item]\n",
    "            gain = c(lista_temp, W, I) - c(V, W, I)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain  # Atualiza o valor de ganho máximo\n",
    "                best_item = item  # Atualiza o melhor item\n",
    "        \n",
    "        if best_item is not None:\n",
    "            V.append(best_item)  # Adiciona o melhor item ao conjunto\n",
    "            nao_selecionados.remove(best_item)  # Remove o melhor item dos não selecionados\n",
    "            itens += 1\n",
    "            c_value += best_gain\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de importância:  [2.         2.         2.23606798]\n",
      "Cobertura:  6.23606797749979\n",
      "Melhor conjunto:  [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "W = np.array([[1, 2, 1], [-1, 0, 2], [-2, 2, -2]])  # Exemplo de matriz W\n",
    "I = importancia(W)\n",
    "print(\"Matriz de importância: \", I)\n",
    "\n",
    "V = [0, 1]  # Lista de índices das instâncias selecionadas\n",
    "print(\"Cobertura: \", c(V, W, I))\n",
    "\n",
    "V_linha = guloso(W, I, 2)\n",
    "print(\"Melhor conjunto: \", V_linha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review  sentimentos\n",
      "0   Este filme foi absolutamente incrível! A trama...            1\n",
      "1   Um desperdício total de tempo. O roteiro é fra...            0\n",
      "2   O roteiro foi bem escrito e os atores entregar...            1\n",
      "3   Não gostei nada da trama. Foi muito previsível...            0\n",
      "4   Uma obra-prima cinematográfica que captura a e...            1\n",
      "..                                                ...          ...\n",
      "93         Totalmente decepcionante. Não vale a pena.            0\n",
      "94   Não esperava isso, impossível parar de assistir.            1\n",
      "95           Viciante, o enrendo te prende até o fim.            1\n",
      "96                                 Uma bela porcaria.            0\n",
      "97  As cenas são muito belas. Filme mais lindo que...            1\n",
      "\n",
      "[98 rows x 2 columns]\n",
      "Accuracy: 0.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      0.93      0.96        14\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.93      0.96      0.94        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n",
      "Probabilidade de sentimento positivo: 0.2935940795687809\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Criar DataFrame\n",
    "dados = pd.read_csv(\"dados.csv\", sep=';')\n",
    "print(dados)\n",
    "\n",
    "# Vetorização\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(dados['review'])\n",
    "y = dados['sentimentos']\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Função para fazer predição em uma nova frase\n",
    "def predict_sentiment(text):\n",
    "    text_vector = vectorizer.transform([text])\n",
    "    prediction = model.predict_proba(text_vector)\n",
    "    return prediction[0][1]  # Retorna a probabilidade de ser positivo\n",
    "\n",
    "# Testando a função\n",
    "new_review = \"O filme é muito lento e chato. Não consegui me conectar com a história em nenhum momento.\"\n",
    "print(\"Probabilidade de sentimento positivo:\", predict_sentiment(new_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para limpar a frase\n",
    "def clean_text(sentence):\n",
    "    sentence = sentence.strip().lower()\n",
    "    sentence = re.sub(r'\\s\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r'[.,\\/#!$%\\^&\\*;:{}=\\-_`~()@?]', '', sentence)\n",
    "    sentence = unicodedata.normalize('NFD', sentence)\n",
    "    sentence = re.sub(r'[\\u0300-\\u036f]', '', sentence)\n",
    "    words = sentence.split()\n",
    "    filtered_words = [word for word in words if len(word) >= 3]\n",
    "    return filtered_words\n",
    "\n",
    "# Define a função para vetorizar a frase\n",
    "def vectorize_sentences(sentence1, sentence2):\n",
    "    sentence1 = clean_text(sentence1)\n",
    "    sentence2 = clean_text(sentence2)\n",
    "    words = list(set(sentence1 + sentence2))\n",
    "    vector1 = np.zeros(len(words))\n",
    "    vector2 = np.zeros(len(words))\n",
    "    for i, word in enumerate(words):\n",
    "        vector1[i] = sentence1.count(word)\n",
    "        vector2[i] = sentence2.count(word)\n",
    "    return vector1, vector2\n",
    "\n",
    "# Define a função de similaridade\n",
    "def similarity(vector1, vector2):\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    \n",
    "def predict_sentiment(text):\n",
    "    text_vector = vectorizer.transform([text])\n",
    "    prediction = model.predict_proba(text_vector)\n",
    "    return prediction[0][1]  # Retorna a probabilidade de ser positivo\n",
    "\n",
    "class LimeExplainerSentences:\n",
    "    def __init__(self, sigma=0.2, num_samples=1000, K=5, alpha=0.1, predict=predict_sentiment, p =10):\n",
    "        self.sigma = sigma\n",
    "        self.num_samples = num_samples\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.predict = predict\n",
    "        self.p = p\n",
    "        \n",
    "    # Define a função de kernel \n",
    "    def kernel(self, x, z):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        distance = similarity(x, z) # Similaridade de cosseno\n",
    "        weights = np.sqrt(np.exp(-(distance**2) / (self.sigma**2)))  # Kernel \n",
    "        return weights\n",
    "        \n",
    "    # Gera dados ao redor de x_line\n",
    "    def samples(self, x):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        x_vec = clean_text(x)\n",
    "        n = len(x_vec)\n",
    "        sample_set = []\n",
    "        for i in range(self.num_samples):\n",
    "            sample = np.zeros(n, dtype=int)\n",
    "            num_ones = np.random.randint(1, self.K)\n",
    "            indices = np.random.choice(n, num_ones, replace=False)\n",
    "            sample[indices] = 1\n",
    "            sample_set.append(sample)\n",
    "        return sample_set\n",
    "    \n",
    "    # Transforma um vetor em uma frase\n",
    "    def sentences_samples(self, x, z_line):\n",
    "        z = []\n",
    "        for i in range(len(clean_text(x))):\n",
    "            if z_line[i] == 1:\n",
    "                z.append(clean_text(x)[i])\n",
    "        return \" \".join(z)\n",
    "\n",
    "    # Define o vetor de pesos\n",
    "    def LIME(self, x):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        Z_line = self.samples(x)\n",
    "        Z = [self.sentences_samples(x, z_line) for z_line in Z_line]\n",
    "        Z_pred = [predict_sentiment(z) for z in Z]\n",
    "        pi_x = []\n",
    "        for z in Z:\n",
    "            x_vec, z_vec = vectorize_sentences(x,z)\n",
    "            pi_x.append(self.kernel(x_vec, z_vec)) \n",
    "        lasso = Lasso(alpha=self.alpha)\n",
    "        lasso.fit(Z_line, Z_pred, sample_weight=pi_x)\n",
    "        w = lasso.coef_\n",
    "        return w\n",
    "    \n",
    "    def K_top_indices(self, x):\n",
    "        abs_valores = np.abs(w)\n",
    "\n",
    "    # Gerar explicação\n",
    "    def explain_instance(self, x):\n",
    "        np.set_printoptions(precision=self.p)\n",
    "        x_vec = clean_text(x)\n",
    "        w = self.LIME(x)\n",
    "        abs_valores = np.abs(w)\n",
    "        indices = np.argsort(abs_valores)[::-1][:self.K]\n",
    "        print(f\"Frase: {x}\")\n",
    "        print(f\"w: {w}\")\n",
    "        print(\"Palavras importantes:\")\n",
    "        for i, word in enumerate(x_vec):\n",
    "            if i in indices:\n",
    "                print(f\"{word}: {w[i]}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instância:  Nunca vi algo tão ruim na minha vida. A produção foi amadora e a história, entediante.\n",
      "Rótulo:  0\n",
      "Frase: Nunca vi algo tão ruim na minha vida. A produção foi amadora e a história, entediante.\n",
      "w: [-0.0251157749  0.0040456922  0.0072241833 -0.1665830075 -0.0224641267\n",
      " -0.0251929131  0.0072716779  0.1450980535 -0.0252221873  0.0072159527\n",
      " -0.0252440058]\n",
      "Palavras importantes:\n",
      "ruim: -0.16658300748884525\n",
      "foi: 0.14509805345196392\n",
      "entediante: -0.025244005783576745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.362e-02, tolerance: 8.745e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Separar as features e o alvo\n",
    "X = dados['review']\n",
    "y = dados['sentimentos']\n",
    "instance_index = 15\n",
    "instance = X.iloc[instance_index]\n",
    "instance_label = y.iloc[instance_index]\n",
    "print(\"Instância: \", instance)\n",
    "print(\"Rótulo: \", instance_label)\n",
    "\n",
    "LIME = LimeExplainerSentences(sigma=0.1, num_samples=1885, K=3, alpha=0.1**(45),  predict=predict_sentiment)\n",
    "LIME.explain_instance(instance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
